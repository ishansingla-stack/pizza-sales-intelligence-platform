# ğŸ–¥ï¸ Local IDE Setup Guide

Run your Pizza Intelligence ML pipeline locally in VS Code/PyCharm while connecting to Databricks for MLflow tracking and compute resources.

---

## âœ… Quick Start (5 Steps)

### Step 1: Install Dependencies
```bash
cd pizza-intelligence
pip install -r requirements.txt
pip install pyyaml  # For config loading
```

### Step 2: Configure Databricks Credentials

1. Copy the template config:
```bash
cp config/databricks_config.yaml.template config/databricks_config.yaml
```

2. Edit `config/databricks_config.yaml` with your details:
```yaml
databricks:
  host: "https://your-workspace.cloud.databricks.com"  # Your workspace URL
  token: "dapi1234567890abcdef"  # Your personal access token
  cluster_id: "optional"  # Only needed for Databricks Connect

mlflow:
  experiment_name: "/Users/your_email@asu.edu/pizza-intelligence"
```

### Step 3: Get Your Databricks Token

1. Log into Databricks
2. Click your user icon (top right) â†’ **User Settings**
3. Click **Developer** â†’ **Access Tokens**
4. Click **Generate New Token**
5. Name: `local-ide`
6. Lifetime: `90 days`
7. Click **Generate**
8. **Copy the token** and paste it in your config file

### Step 4: Update Experiment Name

In `config/databricks_config.yaml`, update with YOUR email:
```yaml
mlflow:
  experiment_name: "/Users/YOUR_EMAIL@example.com/pizza-intelligence"
```

### Step 5: Run the Scripts

```bash
# Run in order:
python scripts/01_data_preparation_local.py
python scripts/02_model_training_regression_local.py
python scripts/03_hyperparameter_tuning_regression_local.py
python scripts/04_model_training_classification_local.py
python scripts/05_clustering_association_local.py
```

---

## ğŸ“ File Structure

```
pizza-intelligence/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ databricks_config.yaml.template  # Template (safe to commit)
â”‚   â””â”€â”€ databricks_config.yaml           # Your config (gitignored!)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_data_preparation_local.py
â”‚   â”œâ”€â”€ 02_model_training_regression_local.py
â”‚   â”œâ”€â”€ 03_hyperparameter_tuning_regression_local.py
â”‚   â”œâ”€â”€ 04_model_training_classification_local.py
â”‚   â””â”€â”€ 05_clustering_association_local.py
â”œâ”€â”€ src/
â”‚   â””â”€â”€ config_loader.py                 # Configuration utility
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ Data_Model_-_Pizza_Sales.xlsx
â”‚   â””â”€â”€ processed/                        # Generated by scripts
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ results/                          # Generated by scripts
â”‚   â””â”€â”€ models/                           # Generated by scripts
â””â”€â”€ .gitignore                            # Protects your secrets!
```

---

## ğŸ”§ How It Works

### Local Execution + Databricks MLflow

When you run scripts locally:
- âœ… **Code runs on your machine** (fast development)
- âœ… **Data is read from local files** (no upload needed)
- âœ… **Results saved locally** (easy to access)
- âœ… **MLflow logs to Databricks** (centralized tracking)
- âœ… **Models registered in Databricks** (production ready)

### What Connects to Databricks?

Only **MLflow tracking**:
- Experiment logging
- Metric tracking
- Model registration
- Run artifacts

### What Runs Locally?

Everything else:
- Data loading
- Feature engineering
- Model training
- Predictions
- File I/O

---

## ğŸš€ Running the Scripts

### Script 1: Data Preparation
```bash
python scripts/01_data_preparation_local.py
```

**What it does:**
- Loads Excel file from `data/raw/`
- Creates 40+ features
- Saves to `data/processed/`
- Logs metadata to MLflow

**Runtime:** ~1 minute

**Outputs:**
- `data/processed/full_features.parquet`
- `data/processed/daily_sales.parquet`
- `data/processed/pizza_features.parquet`
- `data/processed/classification.parquet`

---

### Script 2: Base Model Training (Regression)
```bash
python scripts/02_model_training_regression_local.py
```

**What it does:**
- Trains 15 regression models
- 5-fold cross-validation
- Saves results to `outputs/results/`
- Logs all models to MLflow

**Runtime:** ~5 minutes

**Outputs:**
- `outputs/results/base_models_regression.csv`
- All models logged to MLflow

---

### Script 3: Hyperparameter Tuning
```bash
python scripts/03_hyperparameter_tuning_regression_local.py
```

**What it does:**
- Tunes top 9 models
- Tests 540+ hyperparameter combinations
- Registers best model to MLflow Model Registry
- Saves detailed results

**Runtime:** ~30-60 minutes

**Outputs:**
- `outputs/results/tuned_models_regression.csv`
- Best model in Model Registry

---

### Script 4: Classification Models
```bash
python scripts/04_model_training_classification_local.py
```

**What it does:**
- Trains 10 classification models
- Category and size prediction
- Evaluates with all metrics

**Runtime:** ~5 minutes

**Outputs:**
- `outputs/results/classification_results.csv`

---

### Script 5: Clustering & Association Rules
```bash
python scripts/05_clustering_association_local.py
```

**What it does:**
- K-Means, DBSCAN, Hierarchical, GMM
- Apriori + FP-Growth for bundles
- Support, Confidence, Lift, Leverage

**Runtime:** ~5 minutes

**Outputs:**
- `outputs/results/pizza_clusters.csv`
- `outputs/results/association_rules.csv`

---

## ğŸ“Š View Results

### In MLflow UI (Databricks)

1. Go to your Databricks workspace
2. Click **Machine Learning** â†’ **Experiments**
3. Find your experiment: `/Users/your_email@example.com/pizza-intelligence`
4. View all runs, metrics, models

**URL format:**
```
https://your-workspace.cloud.databricks.com/#mlflow/experiments
```

### In Local Files

All results saved to:
```
outputs/
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ base_models_regression.csv
â”‚   â”œâ”€â”€ tuned_models_regression.csv
â”‚   â”œâ”€â”€ classification_results.csv
â”‚   â”œâ”€â”€ pizza_clusters.csv
â”‚   â””â”€â”€ association_rules.csv
â””â”€â”€ models/
```

---

## ğŸ¯ VS Code Setup

### Recommended Extensions
- Python (Microsoft)
- Pylance
- Jupyter (for .ipynb if needed)

### Run Configuration

Create `.vscode/launch.json`:
```json
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Data Preparation",
            "type": "python",
            "request": "launch",
            "program": "${workspaceFolder}/scripts/01_data_preparation_local.py",
            "console": "integratedTerminal"
        },
        {
            "name": "Model Training",
            "type": "python",
            "request": "launch",
            "program": "${workspaceFolder}/scripts/02_model_training_regression_local.py",
            "console": "integratedTerminal"
        }
    ]
}
```

Then press **F5** to run with debugging!

---

## ğŸ”’ Security Best Practices

### âœ… DO:
- Use `databricks_config.yaml` for local credentials
- Keep `.gitignore` updated
- Use environment variables for CI/CD
- Regenerate tokens every 90 days
- Use separate tokens for dev/prod

### âŒ DON'T:
- Commit `databricks_config.yaml` to Git
- Share your tokens in chat/email
- Use production tokens for development
- Hardcode credentials in scripts

---

## ğŸ› Troubleshooting

### "ModuleNotFoundError: No module named 'src'"
```bash
# Make sure you're in the project root
cd pizza-intelligence
python scripts/01_data_preparation_local.py
```

### "MLflow connection error"
Check your config:
```bash
python src/config_loader.py
```

Should show:
```
âœ… Databricks environment configured
   Host: https://your-workspace.cloud.databricks.com
   MLflow: databricks
```

### "Data file not found"
Make sure your Excel file is at:
```
data/raw/Data_Model_-_Pizza_Sales.xlsx
```

### "Permission denied" when logging to MLflow
- Verify your Databricks token is correct
- Check token hasn't expired
- Ensure experiment name has correct permissions

### Scripts run but don't appear in MLflow
- Check experiment name in config matches Databricks
- Verify `MLFLOW_TRACKING_URI=databricks` is set
- Look for error messages in script output

---

## ğŸ’¡ Tips & Tricks

### Run All Scripts at Once
```bash
# Create a run_all.sh script:
#!/bin/bash
python scripts/01_data_preparation_local.py && \
python scripts/02_model_training_regression_local.py && \
python scripts/03_hyperparameter_tuning_regression_local.py && \
python scripts/04_model_training_classification_local.py && \
python scripts/05_clustering_association_local.py
```

### Use Jupyter Notebooks
Convert scripts to notebooks for interactive development:
```bash
pip install jupytext
jupytext --to notebook scripts/01_data_preparation_local.py
```

### Debug Mode
Add breakpoints in VS Code and use **F5** to debug step-by-step.

### Progress Monitoring
Scripts print progress bars and status messages. Watch the console!

---

## ğŸ†š Local IDE vs Databricks UI

| Feature | Local IDE | Databricks UI |
|---------|-----------|---------------|
| Development | âœ… Fast, familiar | âŒ Browser-based |
| Debugging | âœ… Full IDE support | âš ï¸ Limited |
| Data access | âœ… Local files | âœ… DBFS |
| Compute | ğŸ–¥ï¸ Your machine | â˜ï¸ Databricks cluster |
| MLflow tracking | âœ… Yes | âœ… Yes |
| Model Registry | âœ… Yes | âœ… Yes |
| Collaboration | âš ï¸ Via Git | âœ… Built-in |
| Cost | âœ… Free | ğŸ’° Cluster costs |

**Best of both worlds:** Develop locally, deploy to Databricks!

---

## ğŸ“ Summary

You now have:
- âœ… 5 Python scripts ready to run locally
- âœ… Secure configuration management
- âœ… MLflow integration with Databricks
- âœ… All models and metrics tracked
- âœ… Local file outputs for analysis
- âœ… Git-safe secrets handling

**Next steps:**
1. Configure `databricks_config.yaml`
2. Run scripts in order
3. View results in MLflow UI
4. Analyze local output files

**Total runtime: ~45-70 minutes**

---

**Ready to start? Run:**
```bash
python scripts/01_data_preparation_local.py
```

**Questions? Check the troubleshooting section above!** ğŸš€
